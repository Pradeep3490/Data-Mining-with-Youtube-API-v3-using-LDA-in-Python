{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Importing required libraries\n",
    "from apiclient.discovery import build #pip install google-api-python-client\n",
    "from apiclient.errors import HttpError #pip install google-api-python-client\n",
    "from oauth2client.tools import argparser #pip install oauth2client\n",
    "import pandas as pd #pip install pandas\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Entering Developer Key\n",
    "DEVELOPER_KEY = \”YOUR KEY\” \n",
    "YOUTUBE_API_SERVICE_NAME = \"youtube\"\n",
    "YOUTUBE_API_VERSION = \"v3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Building YouTube object\n",
    "youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION, developerKey=DEVELOPER_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Using Videos().list() to query and retrieve most popular videos by geography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function to loop through a given set of countries and create dataframes containing information on most popular\n",
    "# videos by each country\n",
    "\n",
    "def youtube_mostpopular(c_list):\n",
    "    df = []\n",
    "    for i in range(len(c_list)):\n",
    "        video_list = youtube.videos().list(\n",
    "         part='id,statistics,snippet',\n",
    "         chart = 'mostPopular',\n",
    "         regionCode = c_list[i],\n",
    "         maxResults=50\n",
    "        ).execute()\n",
    "    \n",
    "        for item in video_list['items']:\n",
    "            video_dict = dict(Region = c_list[i] , video_descrp = item['snippet']['description'])\n",
    "            temp_dict = dict(Id =item['id'], Title = item['snippet']['title'])\n",
    "            video_dict.update(temp_dict)\n",
    "            video_dict.update(item['statistics'])\n",
    "            df.append(video_dict)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Defining a list with countries we're interested in\n",
    "country_list = [\"US\",\"AU\",\"NZ\",\"ZA\",\"GB\",\"CA\"]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Calling function\n",
    "df_from_func = youtube_mostpopular(country_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Creating a dataframe from the returned dictionary\n",
    "df = pd.DataFrame.from_dict(df_from_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Region</th>\n",
       "      <th>Title</th>\n",
       "      <th>commentCount</th>\n",
       "      <th>dislikeCount</th>\n",
       "      <th>favoriteCount</th>\n",
       "      <th>likeCount</th>\n",
       "      <th>video_descrp</th>\n",
       "      <th>viewCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cmBvgGbqmzY</td>\n",
       "      <td>US</td>\n",
       "      <td>Everything Wrong With Home Alone In 15 Minutes...</td>\n",
       "      <td>7493</td>\n",
       "      <td>1478</td>\n",
       "      <td>0</td>\n",
       "      <td>46537</td>\n",
       "      <td>After annual frequent requests, we decided to ...</td>\n",
       "      <td>1649451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>haFHrfmfHbc</td>\n",
       "      <td>US</td>\n",
       "      <td>Hallelujah – Pentatonix (From A Pentatonix Chr...</td>\n",
       "      <td>5669</td>\n",
       "      <td>595</td>\n",
       "      <td>0</td>\n",
       "      <td>86544</td>\n",
       "      <td>A PENTATONIX CHRISTMAS OUT NOW! ITUNES http://...</td>\n",
       "      <td>933814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OR2DjKYf8uA</td>\n",
       "      <td>US</td>\n",
       "      <td>Racism is still a huge problem</td>\n",
       "      <td>64395</td>\n",
       "      <td>48577</td>\n",
       "      <td>0</td>\n",
       "      <td>18644</td>\n",
       "      <td>Lady goes off and say racist remarks to Hispan...</td>\n",
       "      <td>2638213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>43RgvVY_wvQ</td>\n",
       "      <td>US</td>\n",
       "      <td>DELTA KICKED US OFF THE PLANE FOR SPEAKING ARA...</td>\n",
       "      <td>99980</td>\n",
       "      <td>100624</td>\n",
       "      <td>0</td>\n",
       "      <td>151274</td>\n",
       "      <td>SLIMS VIDEO: https://youtu.be/duN0mMQ20tc\\n\\nT...</td>\n",
       "      <td>2350827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>QsBT5EQt348</td>\n",
       "      <td>US</td>\n",
       "      <td>Overpopulation – The Human Explosion Explained</td>\n",
       "      <td>13837</td>\n",
       "      <td>1822</td>\n",
       "      <td>0</td>\n",
       "      <td>108140</td>\n",
       "      <td>In a very short amount of time the human popul...</td>\n",
       "      <td>1446040</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Id Region                                              Title  \\\n",
       "0  cmBvgGbqmzY     US  Everything Wrong With Home Alone In 15 Minutes...   \n",
       "1  haFHrfmfHbc     US  Hallelujah – Pentatonix (From A Pentatonix Chr...   \n",
       "2  OR2DjKYf8uA     US                     Racism is still a huge problem   \n",
       "3  43RgvVY_wvQ     US  DELTA KICKED US OFF THE PLANE FOR SPEAKING ARA...   \n",
       "4  QsBT5EQt348     US     Overpopulation – The Human Explosion Explained   \n",
       "\n",
       "  commentCount dislikeCount favoriteCount likeCount  \\\n",
       "0         7493         1478             0     46537   \n",
       "1         5669          595             0     86544   \n",
       "2        64395        48577             0     18644   \n",
       "3        99980       100624             0    151274   \n",
       "4        13837         1822             0    108140   \n",
       "\n",
       "                                        video_descrp viewCount  \n",
       "0  After annual frequent requests, we decided to ...   1649451  \n",
       "1  A PENTATONIX CHRISTMAS OUT NOW! ITUNES http://...    933814  \n",
       "2  Lady goes off and say racist remarks to Hispan...   2638213  \n",
       "3  SLIMS VIDEO: https://youtu.be/duN0mMQ20tc\\n\\nT...   2350827  \n",
       "4  In a very short amount of time the human popul...   1446040  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['US', 'AU', 'NZ', 'ZA', 'GB', 'CA'], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Region.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Performing LDA analysis to discover topics across each region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_US = df.loc[(df.Region==\"US\")]\n",
    "df_GB = df.loc[(df.Region ==\"GB\")]\n",
    "df_AU = df.loc[df.Region==\"AU\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Creating our corpus\n",
    "US_doc = df_US[\"Title\"]\n",
    "GB_doc = df_GB[\"Title\"]\n",
    "AU_doc = df_AU[\"Title\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# create English stop words list\n",
    "en_stop = get_stop_words('en')\n",
    "\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lda(doc):\n",
    "\n",
    "    texts = []\n",
    "    # loop through document list\n",
    "    for i in doc:\n",
    "    # clean and tokenize document string\n",
    "        raw = i.lower()\n",
    "        tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "        # remove stop words from tokens\n",
    "        stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    \n",
    "        # stem tokens\n",
    "        stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    \n",
    "        # add tokens to list\n",
    "        texts.append(stemmed_tokens)\n",
    "\n",
    "        # turn our tokenized documents into a id <-> term dictionary\n",
    "        dictionary = corpora.Dictionary(texts)\n",
    "    \n",
    "    # convert tokenized documents into a document-term matrix\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    \n",
    "    # generate LDA model\n",
    "    ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=2, id2word = dictionary, passes=20)\n",
    "        \n",
    "    # Priting results for US\n",
    "    print(ldamodel.print_topics(num_topics=2, num_words=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.017*\"christma\" + 0.013*\"slow\" + 0.010*\"offici\" + 0.009*\"tie\"'), (1, '0.015*\"christma\" + 0.012*\"trailer\" + 0.012*\"s\" + 0.008*\"chang\"')]\n"
     ]
    }
   ],
   "source": [
    "# Calling lda function for US\n",
    "lda(US_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.021*\"christma\" + 0.017*\"movi\" + 0.014*\"film\" + 0.010*\"make\"'), (1, '0.022*\"christma\" + 0.014*\"plane\" + 0.010*\"new\" + 0.010*\"delta\"')]\n"
     ]
    }
   ],
   "source": [
    "# Calling lda function for Great Britain\n",
    "\n",
    "lda(GB_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.020*\"vs\" + 0.015*\"s\" + 0.009*\"1\" + 0.007*\"christma\"'), (1, '0.020*\"offici\" + 0.014*\"2016\" + 0.011*\"song\" + 0.011*\"video\"')]\n"
     ]
    }
   ],
   "source": [
    "# Calling lda function for Australia\n",
    "\n",
    "lda(AU_doc)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
